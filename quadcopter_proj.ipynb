{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id='b24168be-58d1-4a1b-a018-ac54c912eb20'></div>"
      ],
      "text/plain": [
       "<div id='b24168be-58d1-4a1b-a018-ac54c912eb20'></div>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'NoneType'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-36d08a634a69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ash.mccallum/PycharmProjects/Udacity Project 5 - Quadcopter RL2/task.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, rotor_speeds)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_timestep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotor_speeds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update the sim pose and velocities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_angular_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mpose_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpose_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ash.mccallum/PycharmProjects/Udacity Project 5 - Quadcopter RL2/task.py\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(self, old_angular_v, old_v)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m\"\"\"Uses current pose of sim to return reward.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mdistance_from_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpose\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# punish large deltas in euler angles and velocity to produce smooth flight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0meuler_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_angular_v\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangular_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'NoneType'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# TODO: Train your agent here.\n",
    "import numpy as np\n",
    "from agents.agent123 import Agent\n",
    "from task import Task\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "num_episodes = 100                             # number of episodes\n",
    "init_pose = np.array([0., 0., 5., 0., 0., 0.])   # initial pose\n",
    "init_velocities = np.array([0., 0., 0.])         # initial velocities\n",
    "init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities\n",
    "\n",
    "task = Task(init_pose=init_pose, init_velocities=init_velocities, init_angle_velocities=init_angle_velocities)\n",
    "agent = Agent(task)\n",
    "done = False\n",
    "\n",
    "display_graph = True\n",
    "display_freq = 10\n",
    "\n",
    "# generate plot function\n",
    "def plt_dynamic(x, y1, y2, color_y1='g', color_y2='b'):\n",
    "   sub1.plot(x, y1, color_y1)\n",
    "   sub2.plot(x, y2, color_y2)\n",
    "   fig.canvas.draw()\n",
    "\n",
    "# create plots\n",
    "fig, sub1= plt.subplots(1,1)\n",
    "sub2 = sub1.twinx()\n",
    "\n",
    "# set plot boundaries. y1 = z, y2 = reward\n",
    "time_limit = 5\n",
    "y1_lower = 0\n",
    "y1_upper = 20\n",
    "y2_lower = -100\n",
    "y2_upper = 50\n",
    "\n",
    "sub1.set_xlim(0, time_limit)  # this is typically time\n",
    "sub1.set_ylim(y1_lower, y1_upper)  # limits to your y1\n",
    "sub2.set_xlim(0, time_limit)  # time, again\n",
    "sub2.set_ylim(y2_lower, y2_upper)  # limits to your y2\n",
    "\n",
    "# set labels and colors for the axes\n",
    "sub1.set_xlabel('time (s)', color='k') \n",
    "sub1.set_ylabel('y1-axis label', color='g')\n",
    "sub1.tick_params(axis='x', colors='k')\n",
    "\n",
    "sub1.tick_params(axis='y', colors=\"g\")\n",
    "\n",
    "sub2.set_ylabel('y2-axis label', color='b') \n",
    "sub2.tick_params(axis='y', colors='b')\n",
    "\n",
    "for episode in range(num_episodes + 1):\n",
    "    state = agent.reset_episode()\n",
    "    \n",
    "    x, y1, y2 = [], [], []\n",
    "    \n",
    "    while done is False:\n",
    "        \n",
    "        if (episode % display_freq == 0) and (display_graph is True):\n",
    "            x.append(task.sim.time)\n",
    "            y1.append(task.sim.pose[2])\n",
    "            y2.append(agent.total_reward)\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "    if (episode % display_freq == 0) and (display_graph is True):\n",
    "        plt_dynamic(x, y1, y2)\n",
    "        \n",
    "    print(\"Episode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "        episode, agent.score, agent.best_score, agent.noise_scale))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you must include '%matplotlib notebook' for this to work\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
